---
title: "I've got skills, they're multiplying..."
subtitle: "Source code: [![github logo](github.png){width=100px}](https://github.com/bcgov/skill_value){target='_blank'}"
author: "Richard Martin"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
bibliography: references.bib 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#libraries---------------
library(tidyverse)
library(here)
library(janitor)
library(broom)
library(readxl)
library(partykit)
library(ggparty)
#constants---------------------
jo_start <- "2024"
jo_end <- "2033"
#functions---------------

weighted_average <- function(skills, weights){
  tbbl <- inner_join(skills, weights)|>
    filter(scale_name=="Level")|>
    summarize(score=sum(data_value*weight))|>
    pull(score)
}

get_growth <- function(tbbl){
  start <- tbbl$weighted_average[tbbl$year==min(tbbl$year)]
  end <- tbbl$weighted_average[tbbl$year==max(tbbl$year)]
  end/start-1
}


ave_plot <- function(tbbl, scales, reorder_by){
  subtitle <- if_else(scales=="free", "...arranged by rate of change", "...arranged by levels")
  ggplot(tbbl, aes(year, weighted_average))+
    geom_path(alpha=.5)+
    geom_point(aes(colour=weights))+
    scale_x_continuous(breaks=seq((as.numeric(jo_start)-1), as.numeric(jo_end), by=5))+
    facet_wrap(~fct_reorder(element_name, get(reorder_by), .desc = TRUE), scales = scales)+
    labs(x=NULL,
         y=NULL,
         colour="Weighted by",
    title="Weighted average skills for BC",
    subtitle=subtitle)+    
    theme(legend.position = "bottom")
}

rescale01 <- function(x){
  (x-min(x, na.rm=TRUE))/(max(x, na.rm = TRUE)-min(x, na.rm=TRUE))
}
av_wrapper <- function(term, mod){
  car::avPlot(mod, variable=term)
  recordPlot()
}
get_noc <- function(str){
   temp <- as.data.frame(str_split_fixed(str, " ", 2))
  colnames(temp) <- c("noc","description")
  temp <- temp%>%
    select(-description)
  temp
}
```

```{r, include=FALSE, cache=TRUE}
# the program------------------------
mapping <- read_excel(here("data","onet2019_soc2018_noc2016_noc2021_crosswalk.xlsx"))%>%
  mutate(noc2021=str_pad(noc2021, "left", pad="0", width=5))%>%
  unite(noc, noc2021, noc2021_title, sep=": ")%>%
  select(noc, o_net_soc_code = onetsoc2019)%>%
  distinct()

skills_long <- read_excel(here("data", "Skills.xlsx"))%>%
  clean_names()%>%
  select(o_net_soc_code, element_name, scale_name, data_value)%>%
  inner_join(mapping)%>%
  select(-o_net_soc_code)%>%
  select(noc, everything())%>%
  group_by(noc, element_name, scale_name)%>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) #mapping from SOC to NOC is not one to one: mean give one value per NOC

skills_nested <- skills_long|>
  group_by(element_name)|>
  nest()|>
  rename(skills=data)

#job openings-------------------

jo <- read_excel(here("data", "jo_rich.xlsx"), skip = 3)%>%
  janitor::remove_constant()%>%
  filter(NOC!="#T")%>%
  pivot_longer(cols=starts_with("2"), names_to = "year", values_to = "jo")%>%
  filter(year>=jo_start)%>%
  mutate(NOC=str_replace_all(NOC, "#",""))%>%
  unite(noc, "NOC","Description", sep=": ")
  
jo_weight <- jo|>  
  group_by(noc)%>%
  summarize(job_openings=sum(jo))%>%
  ungroup()%>%
  mutate(job_openings=if_else(job_openings<0,0,job_openings),
    jo_weight=job_openings/sum(job_openings))%>%
  select(-job_openings)

jo_nested <- jo|>
  group_by(year)|>
  mutate(weight=jo/sum(jo))|>
  select(-jo)|>
  nest()
 
#employment----------------------------------------

emp_weight <- vroom::vroom(here("data","employment.csv"), skip = 2)%>%
  filter(Industry=="All industries",
         `Geographic Area`=="British Columbia",
         NOC!="#T")%>%
  mutate(NOC=str_replace_all(NOC, "#",""))%>%
  unite(noc, "NOC","Description", sep=": ")%>%
  mutate(weight=`2023`/sum(`2023`))%>%
  select(noc, weight)

emp_nested <- tibble(year="2023", data=list(emp_weight))
weights_nested <- full_join(emp_nested, jo_nested)|>
  rename(weights=data)

skills_and_weights <- cross_join(skills_nested, weights_nested)|>
  mutate(weighted_average=map2_dbl(skills, weights, weighted_average))|>
  select(-skills, -weights)|>
  group_by(element_name)|>
  mutate(year=as.numeric(year),
         weights=if_else(year >= as.numeric(jo_start), "Job Openings", "Employment"))|>
  nest()|>
  mutate(growth_rate=map_dbl(data, get_growth))|>
  unnest(data)


both <- full_join(emp_weight, jo_weight)|>
  rename(emp_weight=weight)

#income data--------------------------

income_fyft <- read_excel(here("data","median_income_jackie.xlsx"),
                     skip=6,
                    col_names = FALSE,
                     na = c("x","..."))
colnames(income_fyft) <- c("noc","everybody", "income")
income_fyft <- income_fyft%>%
    mutate(nocdf=map(noc, get_noc), .before = noc)%>%
    select(-noc, -everybody)%>%
    unnest(nocdf)

#skills_income-----------------------

skills_income <- skills_long%>%
  separate(noc, into=c("noc","description"), sep = ": ")%>%
  inner_join(income_fyft)|>
  filter(scale_name=="Level")|>
  select(-scale_name)|>
  pivot_wider(names_from = element_name, values_from = data_value)|>
  unite(noc, noc, description, sep=": ")|>
  column_to_rownames(var="noc")|>
  na.omit()|>
  arrange(desc(income))

#income semi-elasticities--------------------------

skills <- skills_long%>%
  pivot_wider(names_from = scale_name, values_from = data_value)|>
  ungroup()|>
  mutate(across(where(is.numeric), ~ 10*rescale01(.x)), #put on same 0-10 scale
         composite=sqrt(Level*Importance))

hedonic_data <- skills%>%
  select(-Importance, -composite)%>%
  pivot_wider(id_cols = noc, names_from = element_name, values_from = Level)%>%
  inner_join(emp_weight)%>%
  separate(noc, into=c("noc","description"), sep = ": ")%>%
  inner_join(income_fyft)%>%
  mutate(income=log(income))%>%
  unite(noc, noc, description, sep=": ")%>%
  column_to_rownames("noc")

weighted <- lm(income~ . -weight, weights = weight, data=hedonic_data)
unweighted <- lm(income~ . -weight,  data=hedonic_data)

sorted_weighted <- weighted%>%
  tidy()%>%
  filter(term!="(Intercept)")%>%
  mutate(percent_change=exp(estimate)-1)%>%
  arrange(desc(estimate))%>%
  mutate(av_plot=map(term, av_wrapper, weighted),
         term=str_replace_all(term, "`",""))

```

![53120: Dancers](grease.mp4)

## Why is skill data important?

1)  Career Planning: Job seekers can use this information to understand the skills needed for different careers and assess whether their current skill set aligns with their desired occupation.

2)  Workforce Development: Educators and training providers can use skill data to design relevant curricula and programs that address the specific skill needs of industries and occupations.

3)  Employee Training: Employers can identify the skills required for specific job roles and develop targeted training programs to enhance the skills of their workforce.

4)  Labor Market Analysis: Policymakers and researchers can use skill data to analyze the labor market and identify trends in demand and supply.

In a nutshell we should care about skills because labour is not fungible: we all differ in terms of the skills, knowledge and experience we possess. Likewise, employers are not simply looking for someone with a pulse: for every position there is a unique bundle of skills, knowledge and experience which would maximize the value of the match. The better the match, the greater the surplus that can be shared between the employer and the employee.

## Outline:

We attempt to answer two questions:

1)  How do BC's future skill needs differ from the present?
2)  What is the relationship between skills and wages?


## ONET Skills

The Occupational Information Network (ONET) is a comprehensive database developed by the U.S. Department of Labor that provides detailed information about various occupations. One of the key components of ONET is the Skills section, which categorizes the skills required for different job roles. ONET's Skills data includes a wide range of skills that are relevant to specific occupations, helping individuals, employers, educators, and policymakers understand the skill requirements of different jobs and industries.

ONET Skills data is organized into several categories:

1)  Basic Skills: These are fundamental skills that are applicable to many different jobs. Examples include reading comprehension, active listening, writing, mathematics, and critical thinking.

2)  Cross-Functional Skills: These skills are more specific than basic skills and may be applicable across various occupations. They include skills like communication, teamwork, problem-solving, adaptability, and creativity.

3)  Technical Skills: These skills are job-specific and relate directly to the tasks and functions of a particular occupation. Technical skills can encompass a wide range of abilities, such as programming, operating machinery, using specialized software, and conducting scientific research.

4)  Job-Related Skills: These skills are directly related to the tasks and responsibilities of a specific job. For instance, a surgeon would require skills like surgical procedures, medical diagnosis, and patient care.

5)  Tools and Technology Skills: This category lists the tools, equipment, and technology that individuals in a particular occupation are expected to use. It includes skills related to operating machinery, software programs, and specialized instruments.

6)  Knowledge Areas: While not skills per se, knowledge areas are also covered in ONET data. These include subjects like mathematics and science that are essential for performing various job functions.

ONET skill data is collected through a combination of methods, including expert analysis, job analysis surveys, occupational research, and data aggregation. The process involves input from various stakeholders, including industry experts, workers, employers, and training providers. Here's an overview of how ONET skill data is collected:

1)  Job Analysis Surveys: Job analysis surveys are a primary method for collecting data on the skills required for different occupations. ONET relies on surveys administered to workers, supervisors, and subject matter experts who are familiar with the tasks, responsibilities, and skill requirements of specific jobs. These surveys gather information about the skills that are critical to performing job tasks effectively.

2)  Subject Matter Experts (SMEs): ONET collaborates with subject matter experts who have in-depth knowledge and experience in various industries and occupations. These experts provide valuable insights into the skills that are most relevant and important for specific job roles.

3)  Occupational Research: ONET researchers study various industries and occupations to identify emerging trends, technological advancements, and changes in skill requirements. This research helps ensure that ONET's skill data remains current and aligned with the evolving demands of the labor market.

4)  Existing Data Sources: ONET also aggregates information from existing sources, such as industry reports, job postings, and training programs. This data helps supplement the skill information gathered through surveys and expert input.

## Job Analysis Survey

Workers, supervisors and subject matter experts are asked about the importance and level of the 35 different skills: e.g.

![](onet_question.png)


## Future skill needs

The future demand for skills may differ from the current demand for skills in 3 ways:

1)  The skill profile of an occupation may change over time.
2)  The set of occupations may change over time (e.g. a new occupation is introduced).
3)  The occupation mix (i.e. proportion of people in each occupation) may change over time.

There is not much that we can do to predict skill changes due to 1) or 2), but we can investigate 3) by using data from the BC Labour Market Outlook (LMO). The LMO forecasts job openings over the next 10 years. In what follows, we take the skill data for these 500ish occupations and create a weighted average for each skill level. The weights are employment for the current year, and job openings for each of the following 10 years.

```{r, fig.width=16, fig.height=9}
ave_plot(skills_and_weights, "free", "growth_rate")
```

In the plot above it appears that the average skill profile is changing rapidly over time for many skills, with most skills showing an increase in their weighted average level. However note the scale of the y-axis... Here is another view where all the plots share the same y-axis, which includes the origin.

```{r, fig.width=16, fig.height=9}
ave_plot(skills_and_weights, "fixed", "weighted_average")
```

The stability in the skill profile over time is due to the fact that current employment and future job openings are highly correlated, implying that the weights are not changing much over time.

## Relationship between skill level and wage rate (income for now, wages come November)

One way to think about workers is that we are just a bundle of attributes (skills, knowledge, experience) that employers want to rent. If the goal of a job seeker is to maximize their wages, then it makes sense to invest in the skills that are most highly valued by employers. First thing we do is plot the skills data:

```{r, fig.width=10, fig.height=6}
skills_income|>
  select(-income)|>
  t()|>
  as.data.frame()|>
  heatmaply::heatmaply(dendrogram = "row", main = "Occupations (sorted by income) vs. Skills (clustered)", showticklabels=c(FALSE, FALSE))
```

Take aways:

1)  There are two main clusters of skills, which can be broadly characterized as generic (bottom) vs. specific (top).
    -   Generic skills have a fairly consistent colour of green: these skills are important across all occupations.
    -   Specific skills have a more variable blueish colour: these skills are not as important in general, but they *are* important for certain occupations.
2)  There appears to be a relationship between skills and income:
    -   The occupations are organized by income, with high income to the left, where skill levels are higher as well.
3)  Highly paid professions where the generic skills are relatively low have relatively high specific skills:
    -   Graphically this corresponds to a vertical stripe towards the left of the plot, where at the bottom it is darker (relatively low levels of generic skills) and the top is lighter (relatively high levels of specific skills)

The above suggests that we might want to look more closely at the pairwise correlations between the skills and income. In the plot below the variables are organized in terms of their correlation with income.

```{r, fig.width=10, fig.height=6}
skills_cor <- cor(skills_income)|>
  as.data.frame()|>
  arrange(desc(income))

skills_cor <- skills_cor|>
  select(row.names(skills_cor))

heatmaply::heatmaply(skills_cor,
                     dendrogram = "none",
                     main = "Pairwise correlations between income and skills",
                     showticklabels=c(FALSE, FALSE),
                     key.title = "Correlation"
                     )
```

Take aways:

1)  Consistent with the previous heatmap of the raw data, we see two distinct groups of skills, where scores are highly correlated within a group (yellow), but negatively correlated between groups (blue).
2)  The generic skills are more highly correlated with income than the specific skills.
3)  Certain skill measures might be redundant: e.g. correlation between repairing and equipement maintenance is .99

## Hedonic estimation:

If we are willing to assume that skill interactions are *not* an important determinant of wages, we can use hedonic estimation to estimate the wage semi-elasticities with the model:

$$log(wage)=\hat{\beta_0}+\sum^{35}_{i=1}\hat{\beta_i} \times level_i + \epsilon$$ i.e. holding all other skills constant, a 1 unit increase in skill level $i$ is associated with a $100\times (e^{\hat{\beta_i}}-1)$% change in wage rate. In order to interpret these as causal effects, one must be willing to assume that for a given occupation the associated skill profile is "as good as" randomly assigned, which precludes

1)  Confounding variables that influence both skills and wages.
2)  Reverse causality, where the wage rate of an occupation influences its skill profile.

## Regression results

We run two models: first an un-weighted plain vanilla OLS regression (every occupation is equally important), and second a weighted least squares regression, where the weights are the current employment levels: This puts a large weight on occupations where there are many people with that occupation, and a small weight on occupations where there are few.

```{r, results='asis'}
models <- list(OLS=unweighted, WLS=weighted)
texreg::htmlreg(models, 
                custom.coef.names=str_replace_all(names(weighted[["coefficients"]]),"`",""),
                caption="Regression Results")
```

## Diagnostic plots of weighted regression

```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2))
plot(weighted, ask = FALSE)
```

-   Residuals vs. Fitted: no obvious relationship between residuals and fitted values, which is good.
-   Q-Q residuals: Non normal residuals (fat tailed distribution), which is bad for inference, but because we have more than 10 observations per parameter we are ok: @schmidt2018linear
-   Scale-Location: looks ok, but might as well check for heteroskedasticity... all good.

```{r, results='asis'}
lmtest::bptest(weighted)
```

-   Residuals vs. Leverage: No residuals with a Cook's distance greater than .5. Outliers not a problem.

## Effect size

Below are the effect sizes, with the regression p.values determining shade (dark \~ low p.value)

```{r}
library(RColorBrewer)
ggplot(sorted_weighted, aes(x=percent_change,
                            y=fct_reorder(term,percent_change),
                            fill=p.value
                            )
       )+
  geom_col()+
  scale_fill_distiller()+
  scale_x_continuous(labels=scales::percent)+
  labs(x="Change in income associated with 1 unit increase in skill level",
       y="",
       title="Hedonic Estimates")+
  theme_minimal()
```

## Residuals analysis:

Skill is not the only factor that determines wages, and any omitted factors gets captured in the residual. Note that the adjusted $R^2=$ `r round(glance(weighted)$adj.r.squared,2)` for the weighted regression, implying `r scales::percent(1-glance(weighted)$adj.r.squared, accuracy=1)` of the variation in wage is *not* explained by skill.

One interpretation of the residuals is that they capture what is know as compensating wage differentials [@smith1776inquiry]. Unpleasant or dangerous jobs require a greater wage than skill alone would predict, the opposite occurring for fun jobs. The next plot shows the occupations with the largest absolute value residuals from the weighted regression.

```{r}
residuals <- enframe(weighted$residuals)|>
  mutate(noc=str_sub(name,1,1),
         teer=str_sub(name,2,2))

plt <- residuals|>
  filter(abs(value)>.5)|>
  ggplot(aes(value, fct_reorder(name,value), text=name))+geom_point(alpha=.25)+
     theme(axis.title.y =element_blank(),
           axis.text.y=element_blank(),
           axis.ticks.y=element_blank())+
  labs(title="The occupations with the largest residuals",
    x="residuals")
 
 plotly::ggplotly(plt, tooltip = "text")
```

Another possibility is that educational attainment influences wages even once we control for skill: i.e. the piece of paper matters. To investigate this possibility we plot the residuals from the skill regression vs TEER.

```{r}
ggplot(residuals, aes(value, teer))+
  geom_vline(xintercept = 0, col="grey", lwd=1.5)+
  geom_boxplot(fill="red", alpha=.25, outlier.shape = NA)+
  geom_jitter(alpha=.25)+
  theme_minimal()+
  labs(title="Controling for skills, TEER 0 (Management) appears to be overpaid.",
       x="Residuals from regressing wage on skill",
       y="TEER")
```

There is no apparent relationship between TEER and the residuals for TEERs 1-5, but TEER 0 is an outlier, in the sense that their wages are systematically higher than their skill profile would suggest. Some possible explanations:

-   Management jobs are typically not unionized: higher risk of termination.
-   Management jobs are typically not 9-5: lack of work/life balance.

## Skill interactions:

As alluded to above, the hedonic analysis above assumes that skill interactions are not important: it only captures the direct effect of skill changes. One could easily imagine complementarities between skills: e.g. increasing two skills in tandem yields a greater increase in wage than if the skills increased individually. To allow for these types of interactions we move to a new modeling technique, decision trees.

### Conditional inference trees

The algorithm utilized below is a statistical and machine learning method used for building decision trees. It is designed to create unbiased decision trees by using statistical significance as a decision criteria. Here's how the algorithm works:

1)  Select a splitting variable: The algorithm chooses the skill that is most closely related to income.

2)  Split the data: The algorithm splits the data (by the variable above) in such a way that maximizes the differences in wages between the two resulting groups.

3)  Test the Split: The algorithm then tests if the split is statistically significant. In other words, it checks if the division of the data into these groups provides meaningful information for making predictions.

4)  Repeat the Process: If the split is significant, the algorithm continues to divide the data further, selecting the "best" variable to split on at each step. This process is repeated recursively until it decides that further splits are not significantly different.

Suppose that we had only two explanatory variables, $X1$ and $X2$, and the response is $R$. A decision tree might look something like this:

![](tree_and_space.png)


The fact that decision trees allow for interactions between the explanatory variables can be seen most easily in the 3-D representation of the model:

![](two_and_three_D.png)

The key idea behind the Conditional Inference Tree algorithm is that it uses statistical tests at each step to ensure that the splits are based on meaningful patterns in the data, which leads to more accurate decision trees: less bias, less variance (over-fitting). Here is a conditional inference tree based on our data:

```{r, fig.height=9, fig.width=16}
skill_tree <- ctree(income~., data=skills_income)

ggparty(skill_tree,
        add_vars = list(mean_income = function(data, node) scales::dollar(mean(node$data$income), accuracy = 1)))+
  geom_edge() +
  geom_edge_label(mapping = aes(label = paste(substr(breaks_label, start = 1, stop = 15))))+
  geom_node_label(
    line_list = list(
      aes(label = splitvar),
      aes(label = paste("N =", nodesize)),
      aes(label = paste("p =",
                        formatC(p.value,
                                format = "e",
                                digits = 2)))
    ),
    line_gpar = list(
      list(size = 13),
      list(size = 10),
      list(size = 10)
    ),
    ids = "inner"
  ) +
  geom_node_label(aes(label = paste0("Average = ", mean_income, "\n N = ", nodesize)),
                  ids = "terminal", nudge_y = -0.3, nudge_x = 0.01
  ) +
  geom_node_plot(
    gglist = list(
      geom_boxplot(aes(x="", y=income), fill="red", alpha=.5, outlier.shape = NA),
      geom_jitter(aes(x="", y=income), alpha=.2, height=0),
      scale_y_continuous(trans="log10", labels = scales::dollar),
      labs(x="", y=""),
      theme_minimal()
    )
  )
```

Take aways:

1)  Complex problem solving is the skill that is the "best" predictor of wage.
2)  Operations monitoring, systems analysis, and operations and control also are significant predictors of wage.

A problem with decision trees is that they are not very robust: a small change in the data can lead to a very different tree. To overcome this issue, we move on to our next modelling technique:

### Random forests

Rather than utilizing a single decision tree, we can create a forest of trees. For each tree in the forest, we introduce some randomness:

1)  Each tree is based on a different random subset of occupations.
2)  For each decision node, only a random subset of skills are considered.

Once we have created a forest of trees we can use this "ensemble" to predict wages. The act of averaging across many trees reduces the variance when compared to a single decision tree. One of the outputs of a random forest is a measure of variable importance: How important was each skill (on average) in predicting wage.

```{r, cache=TRUE}
set.seed(123)
skill_forest <- cforest(income~., data=skills_income, mtry = 10, ntree = 500, cores = 3)
var_imp <- varimp(skill_forest)
var_imp <- tibble(skill=names(var_imp), importance=var_imp)

ggplot(var_imp, aes(x=importance, y=fct_reorder(skill, importance)))+
  geom_col(alpha=.5)+
  labs(y=NULL,
       x="Skill Importance",
       title="Random Forest variable importance plot.")+
  theme_minimal()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

# References
